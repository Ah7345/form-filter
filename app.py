import streamlit as st
import io
import zipfile
from docx import Document
import re

# Utility functions for parsing and cleaning
def normalize_digits(s):
    """Convert Arabic-Indic digits to ASCII digits."""
    if not s:
        return s
    
    # Arabic-Indic to ASCII mapping
    arabic_digits = {
        'Ù ': '0', 'Ù¡': '1', 'Ù¢': '2', 'Ù£': '3', 'Ù¤': '4',
        'Ù¥': '5', 'Ù¦': '6', 'Ù§': '7', 'Ù¨': '8', 'Ù©': '9'
    }
    
    for arabic, ascii_digit in arabic_digits.items():
        s = s.replace(arabic, ascii_digit)
    
    return s

def clean_value(s):
    """Clean and normalize values by removing duplicates, prefixes, and normalizing formatting."""
    if not s:
        return ""
    
    # Normalize digits
    s = normalize_digits(s)
    
    # Split by lines and clean each line
    lines = s.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Remove repeated prefixes like "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:" etc.
        prefixes_to_remove = [
            r'^(\s*Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙˆØ­Ø¯Ø§Øª\s*[:ï¼š]\s*)+',
            r'^(\s*Ø±Ù…Ø² Ø§Ù„ÙˆØ­Ø¯Ø§Øª\s*[:ï¼š]\s*)+',
            r'^(\s*Ø§Ù„Ù…Ù‡Ù†Ø©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø±Ù…Ø² Ø§Ù„Ù…Ù‡Ù†Ø©\s*[:ï¼š]\s*)+',
            r'^(\s*Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ù…Ù„\s*[:ï¼š]\s*)+',
            r'^(\s*Ø§Ù„Ù…Ø±ØªØ¨Ø©\s*[:ï¼š]\s*)+'
        ]
        
        for prefix_pattern in prefixes_to_remove:
            line = re.sub(prefix_pattern, '', line)
        
        # Normalize separators (Arabic semicolons/commas)
        line = line.replace('Ø›', ';').replace('ØŒ', ',')
        
        # Collapse multiple spaces
        line = re.sub(r'\s+', ' ', line).strip()
        
        if line and line not in cleaned_lines:
            cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

def extract_reference_data(text):
    """Extract reference data for 'Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ù„Ù„Ù…Ù‡Ù†Ø©' section."""
    expected_labels = [
        "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©", "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©",
        "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©", "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©", "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙˆØ­Ø¯Ø§Øª", "Ø±Ù…Ø² Ø§Ù„ÙˆØ­Ø¯Ø§Øª",
        "Ø§Ù„Ù…Ù‡Ù†Ø©", "Ø±Ù…Ø² Ø§Ù„Ù…Ù‡Ù†Ø©", "Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ù…Ù„", "Ø§Ù„Ù…Ø±ØªØ¨Ø©"
    ]
    
    # Build regex pattern for all labels
    label_pattern = '|'.join(map(re.escape, expected_labels))
    pattern = rf'^\s*({label_pattern})\s*[:ï¼š]\s*(.+)$'
    
    extracted_data = {}
    lines = text.split('\n')
    
    for line in lines:
        match = re.match(pattern, line)
        if match:
            label = match.group(1)
            value = clean_value(match.group(2))
            if label not in extracted_data and value:
                extracted_data[label] = value
    
    # Ensure all expected labels exist (fill with empty if missing)
    result = {}
    for label in expected_labels:
        result[label] = extracted_data.get(label, "")
    
    return result

def extract_communication_channels(text):
    """Extract internal and external communication channels."""
    internal_channels = []
    external_channels = []
    
    lines = text.split('\n')
    current_section = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        if 'Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©' in line or 'Ø¯Ø§Ø®Ù„ÙŠ' in line:
            current_section = 'internal'
            # Extract channels from this line
            channels_text = re.sub(r'^.*?[:ï¼š]\s*', '', line)
            if channels_text:
                channels = [c.strip() for c in re.split(r'[,ØŒ;Ø›]', channels_text) if c.strip()]
                internal_channels.extend(channels)
        elif 'Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©' in line or 'Ø®Ø§Ø±Ø¬ÙŠ' in line:
            current_section = 'external'
            # Extract channels from this line
            channels_text = re.sub(r'^.*?[:ï¼š]\s*', '', line)
            if channels_text:
                channels = [c.strip() for c in re.split(r'[,ØŒ;Ø›]', channels_text) if c.strip()]
                external_channels.extend(channels)
        elif 'Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„ØªÙˆØ§ØµÙ„' in line and current_section:
            purpose_text = re.sub(r'^.*?[:ï¼š]\s*', '', line)
            if purpose_text:
                purpose = clean_value(purpose_text)
                if current_section == 'internal' and internal_channels:
                    # Associate purpose with internal channels
                    pass  # Will be handled in the table creation
                elif current_section == 'external' and external_channels:
                    # Associate purpose with external channels
                    pass  # Will be handled in the table creation
    
    return {
        'internal': internal_channels,
        'external': external_channels
    }

def extract_profession_levels(text):
    """Extract profession levels information."""
    levels_data = {}
    
    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        if 'Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†Ø©' in line:
            levels_data['Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ'] = clean_value(re.sub(r'^.*?[:ï¼š]\s*', '', line))
        elif 'Ø±Ù…Ø² Ø§Ù„Ù…Ø³ØªÙˆÙ‰' in line:
            levels_data['Ø±Ù…Ø² Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†ÙŠ'] = clean_value(re.sub(r'^.*?[:ï¼š]\s*', '', line))
        elif 'Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ' in line:
            levels_data['Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ'] = clean_value(re.sub(r'^.*?[:ï¼š]\s*', '', line))
        elif 'Ø§Ù„ØªØ±ØªÙŠØ¨' in line or 'Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„Ù…Ù‡Ù†ÙŠ' in line:
            levels_data['Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„Ù…Ù‡Ù†ÙŠ (Ø§Ù„Ù…Ø±ØªØ¨Ø©)'] = clean_value(re.sub(r'^.*?[:ï¼š]\s*', '', line))
    
    # Ensure all expected fields exist
    expected_fields = ['Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ', 'Ø±Ù…Ø² Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†ÙŠ', 'Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ', 'Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„Ù…Ù‡Ù†ÙŠ (Ø§Ù„Ù…Ø±ØªØ¨Ø©)']
    for field in expected_fields:
        if field not in levels_data:
            levels_data[field] = ""
    
    return levels_data

def extract_competencies(text):
    """Extract competencies split by type."""
    competencies = {
        'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©': [],
        'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©': [],
        'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©': [],
        'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©': []
    }
    
    lines = text.split('\n')
    current_type = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Determine competency type
        if 'Ø³Ù„ÙˆÙƒÙŠØ©' in line:
            current_type = 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©'
        elif 'Ø£Ø³Ø§Ø³ÙŠØ©' in line:
            current_type = 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©'
        elif 'Ù‚ÙŠØ§Ø¯ÙŠØ©' in line:
            current_type = 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©'
        elif 'ÙÙ†ÙŠØ©' in line:
            current_type = 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©'
        elif current_type and ':' in line:
            # Extract competencies from this line
            comp_text = re.sub(r'^.*?[:ï¼š]\s*', '', line)
            if comp_text:
                comps = [c.strip() for c in re.split(r'[,ØŒ;Ø›]', comp_text) if c.strip()]
                competencies[current_type].extend(comps)
    
    return competencies

def extract_tasks(text):
    """Extract tasks split by category."""
    tasks = {
        'Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©/Ø§Ù„Ø¥Ø´Ø±Ø§ÙÙŠØ©': [],
        'Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ®ØµØµÙŠØ©': [],
        'Ù…Ù‡Ø§Ù… Ø£Ø®Ø±Ù‰ Ø¥Ø¶Ø§ÙÙŠØ©': []
    }
    
    lines = text.split('\n')
    current_category = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Determine task category
        if 'Ù‚ÙŠØ§Ø¯ÙŠØ©' in line or 'Ø¥Ø´Ø±Ø§ÙÙŠØ©' in line:
            current_category = 'Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©/Ø§Ù„Ø¥Ø´Ø±Ø§ÙÙŠØ©'
        elif 'ØªØ®ØµØµÙŠØ©' in line:
            current_category = 'Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ®ØµØµÙŠØ©'
        elif 'Ù…Ù‡Ø§Ù… Ø£Ø®Ø±Ù‰' in line or 'Ø¥Ø¶Ø§ÙÙŠØ©' in line:
            current_category = 'Ù…Ù‡Ø§Ù… Ø£Ø®Ø±Ù‰ Ø¥Ø¶Ø§ÙÙŠØ©'
        elif current_category and ':' in line:
            # Extract tasks from this line
            task_text = re.sub(r'^.*?[:ï¼š]\s*', '', line)
            if task_text:
                task_list = [t.strip() for t in re.split(r'[,ØŒ;Ø›]', task_text) if t.strip()]
                tasks[current_category].extend(task_list)
    
    return tasks

def extract_kpis(text):
    """Extract KPIs with measurement methods."""
    kpis = []
    
    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Look for KPI patterns like "1- {kpi} - Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³: {method}"
        kpi_match = re.match(r'^(\d+)[-Ù€]\s*(.+?)\s*[-Ù€]\s*Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³\s*[:ï¼š]\s*(.+)$', line)
        if kpi_match:
            kpis.append({
                'Ø§Ù„Ø±Ù‚Ù…': kpi_match.group(1),
                'Ù…Ø¤Ø´Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡': clean_value(kpi_match.group(2)),
                'Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³': clean_value(kpi_match.group(3))
            })
    
    # If no structured KPIs found, try to extract from general text
    if not kpis:
        kpi_text = clean_value(text)
        if kpi_text:
            kpis = [{'Ø§Ù„Ø±Ù‚Ù…': '1', 'Ù…Ø¤Ø´Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡': kpi_text, 'Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³': 'Ù‚ÙŠØ§Ø³ Ù…Ø¨Ø§Ø´Ø±'}]
    
    return kpis

# DOCX building helper functions
def add_title(doc, text):
    """Add a centered, bold title."""
    title = doc.add_heading(text, 0)
    title.alignment = 1  # Center alignment
    return title

def add_keyval_table(doc, title, rows):
    """Add a 2-column key-value table."""
    if not rows:
        return
    
    # Create table
    table = doc.add_table(rows=len(rows) + 1, cols=2)
    table.style = 'Table Grid'
    
    # Header row
    header_cells = table.rows[0].cells
    header_cells[0].text = title
    header_cells[1].text = "Ø§Ù„Ù‚ÙŠÙ…Ø©"
    
    # Make header bold and center
    for cell in header_cells:
        for paragraph in cell.paragraphs:
            paragraph.alignment = 1  # Center
            for run in paragraph.runs:
                run.bold = True
    
    # Data rows
    for i, (key, value) in enumerate(rows.items() if isinstance(rows, dict) else enumerate(rows)):
        row_cells = table.rows[i + 1].cells
        if isinstance(rows, dict):
            row_cells[0].text = str(key)
            row_cells[1].text = str(value) if value else ""
        else:
            row_cells[0].text = str(key)
            row_cells[1].text = str(value) if value else ""
        
        # Make label column bold
        for paragraph in row_cells[0].paragraphs:
            for run in paragraph.runs:
                run.bold = True
    
    # Set RTL alignment for all cells
    for row in table.rows:
        for cell in row.cells:
            for paragraph in cell.paragraphs:
                paragraph.alignment = 2  # RTL alignment
    
    # Add spacing after table
    doc.add_paragraph("")

def add_list_table(doc, title, items):
    """Add a single-column list table."""
    if not items:
        return
    
    # Create table
    table = doc.add_table(rows=len(items) + 1, cols=1)
    table.style = 'Table Grid'
    
    # Header row
    header_cell = table.rows[0].cells[0]
    header_cell.text = title
    
    # Make header bold and center
    for paragraph in header_cell.paragraphs:
        paragraph.alignment = 1  # Center
        for run in paragraph.runs:
            run.bold = True
    
    # Data rows
    for i, item in enumerate(items):
        row_cell = table.rows[i + 1].cells[0]
        row_cell.text = str(item)
    
    # Set RTL alignment for all cells
    for row in table.rows:
        for cell in row.cells:
            for paragraph in cell.paragraphs:
                paragraph.alignment = 2  # RTL alignment
    
    # Add spacing after table
    doc.add_paragraph("")

def add_two_col_table(doc, title, rows):
    """Add a two-column table for communication channels."""
    if not rows:
        return
    
    # Create table
    table = doc.add_table(rows=len(rows) + 1, cols=2)
    table.style = 'Table Grid'
    
    # Header row
    header_cells = table.rows[0].cells
    header_cells[0].text = title
    header_cells[1].text = "Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„ØªÙˆØ§ØµÙ„"
    
    # Make header bold and center
    for cell in header_cells:
        for paragraph in cell.paragraphs:
            paragraph.alignment = 1  # Center
            for run in paragraph.runs:
                run.bold = True
    
    # Data rows
    for i, row_data in enumerate(rows):
        row_cells = table.rows[i + 1].cells
        if isinstance(row_data, dict):
            row_cells[0].text = str(row_data.get('Ø¬Ù‡Ø©', ''))
            row_cells[1].text = str(row_data.get('ØºØ±Ø¶', ''))
        else:
            row_cells[0].text = str(row_data[0]) if len(row_data) > 0 else ""
            row_cells[1].text = str(row_data[1]) if len(row_data) > 1 else ""
    
    # Set RTL alignment for all cells
    for row in table.rows:
        for cell in row.cells:
            for paragraph in cell.paragraphs:
                paragraph.alignment = 2  # RTL alignment
    
    # Add spacing after table
    doc.add_paragraph("")

def add_kpi_table(doc, title, rows):
    """Add a 3-column KPI table."""
    if not rows:
        return
    
    # Create table
    table = doc.add_table(rows=len(rows) + 1, cols=3)
    table.style = 'Table Grid'
    
    # Header row
    header_cells = table.rows[0].cells
    header_cells[0].text = "Ø§Ù„Ø±Ù‚Ù…"
    header_cells[1].text = "Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"
    header_cells[2].text = "Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³"
    
    # Make header bold and center
    for cell in header_cells:
        for paragraph in cell.paragraphs:
            paragraph.alignment = 1  # Center
            for run in paragraph.runs:
                run.bold = True
    
    # Data rows
    for i, row_data in enumerate(rows):
        row_cells = table.rows[i + 1].cells
        if isinstance(row_data, dict):
            row_cells[0].text = str(row_data.get('Ø§Ù„Ø±Ù‚Ù…', i + 1))
            row_cells[1].text = str(row_data.get('Ù…Ø¤Ø´Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡', ''))
            row_cells[2].text = str(row_data.get('Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³', ''))
        else:
            row_cells[0].text = str(i + 1)
            row_cells[1].text = str(row_data[0]) if len(row_data) > 0 else ""
            row_cells[2].text = str(row_data[1]) if len(row_data) > 1 else ""
    
    # Set RTL alignment for all cells
    for row in table.rows:
        for cell in row.cells:
            for paragraph in cell.paragraphs:
                paragraph.alignment = 2  # RTL alignment
    
    # Add spacing after table
    doc.add_paragraph("")

def build_filled_docx_bytes(template_bytes: bytes, job_title: str, data: dict) -> bytes:
    """
    Build a filled DOCX using structured tables following the exact template design.
    This creates clean, professional documents with proper table structures.
    """
    try:
        # Create a new document
        doc = Document()
        
        # Add title - centered and bold
        title = add_title(doc, f"Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ÙˆØµÙ Ø§Ù„Ù…Ù‡Ù†ÙŠ â€” {job_title}")
        
        # Add spacing after title
        doc.add_paragraph("")
        
        # Section 1: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ù„Ù„Ù…Ù‡Ù†Ø© (Job Reference Data)
        ref_data = extract_reference_data(data.get("ref", ""))
        # Override with job title
        ref_data["Ø§Ù„Ù…Ù‡Ù†Ø©"] = job_title
        add_keyval_table(doc, "1- Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ù„Ù„Ù…Ù‡Ù†Ø©", ref_data)
        
        # Section 2: Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù…Ù‡Ù†Ø© (General Summary)
        summary_data = clean_value(data.get("summary", "")) or "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ø®Øµ"
        add_keyval_table(doc, "2- Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù…Ù‡Ù†Ø©", {"Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù…": summary_data})
        
        # Section 3: Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ (Communication Channels)
        channels = extract_communication_channels(data.get("channels", ""))
        
        # Internal channels table
        if channels['internal']:
            internal_rows = [{'Ø¬Ù‡Ø©': channel, 'ØºØ±Ø¶': 'ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„'} for channel in channels['internal']]
            add_two_col_table(doc, "3- Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©", internal_rows)
        
        # External channels table
        if channels['external']:
            external_rows = [{'Ø¬Ù‡Ø©': channel, 'ØºØ±Ø¶': 'Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡'} for channel in channels['external']]
            add_two_col_table(doc, "3- Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©", external_rows)
        
        # Section 4: Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ© (Standard Profession Levels)
        levels_data = extract_profession_levels(data.get("levels", ""))
        add_keyval_table(doc, "4- Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ©", levels_data)
        
        # Section 5: Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª (Competencies)
        competencies = extract_competencies(data.get("competencies", ""))
        for comp_type, comp_list in competencies.items():
            if comp_list:
                add_list_table(doc, f"5- {comp_type}", comp_list)
        
        # Section 6: Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù†ÙŠ (Performance Management)
        kpis = extract_kpis(data.get("kpis", ""))
        if kpis:
            add_kpi_table(doc, "6- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù†ÙŠ", kpis)
        
        # Section 7: Ø§Ù„Ù…Ù‡Ø§Ù… (Tasks)
        tasks = extract_tasks(data.get("tasks", ""))
        for task_type, task_list in tasks.items():
            if task_list:
                add_list_table(doc, f"7- {task_type}", task_list)
        
        # Add Form B: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙˆØµÙ Ø§Ù„ÙØ¹Ù„ÙŠ (Actual Description Form)
        doc.add_heading("Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙˆØµÙ Ø§Ù„ÙØ¹Ù„ÙŠ", level=1)
        doc.add_paragraph("")
        
        # Form B Section 1: Ø§Ù„Ù…Ù‡Ø§Ù… (Tasks)
        if tasks:
            for task_type, task_list in tasks.items():
                if task_list:
                    add_list_table(doc, f"1- {task_type}", task_list)
        
        # Form B Section 2: Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ© ÙˆØ§Ù„ÙÙ†ÙŠØ© (Behavioral and Technical Competencies)
        behavioral_comps = competencies.get('Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©', [])
        technical_comps = competencies.get('Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©', [])
        
        if behavioral_comps:
            behavioral_rows = [{'Ø§Ù„Ø±Ù‚Ù…': i+1, 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©': comp, 'Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†': 'Ù…ØªÙ‚Ø¯Ù…'} 
                             for i, comp in enumerate(behavioral_comps[:5])]
            add_kpi_table(doc, "2- Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ© ÙˆØ§Ù„ÙÙ†ÙŠØ©", behavioral_rows)
        
        if technical_comps:
            technical_rows = [{'Ø§Ù„Ø±Ù‚Ù…': i+1, 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©': comp, 'Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†': 'Ù…ØªÙ‚Ø¯Ù…'} 
                            for i, comp in enumerate(technical_comps[:5])]
            add_kpi_table(doc, "2- Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©", technical_rows)
        
        # Form B Section 3: Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù†ÙŠ (Performance Management)
        if kpis:
            add_kpi_table(doc, "3- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù†ÙŠ", kpis)
        
        # Save the rendered document to bytes
        out = io.BytesIO()
        doc.save(out)
        out.seek(0)
        return out.read()
        
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ù„Ø¡ Ø§Ù„Ù‚Ø§Ù„Ø¨: {e}")
        return template_bytes

def zip_many(named_bytes: dict[str, bytes]) -> bytes:
    bio = io.BytesIO()
    with zipfile.ZipFile(bio, 'w') as zf:
        for name, data in named_bytes.items():
            zf.writestr(name, data)
    bio.seek(0)
    return bio.read()

def read_docx_paragraphs(file_bytes: bytes) -> str:
    """Read DOCX file and return all text content."""
    try:
        doc = Document(io.BytesIO(file_bytes))
        text_content = ""
        
        # Read paragraphs
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                text_content += paragraph.text.strip() + "\n"
        
        # Read tables
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    if cell.text.strip():
                        text_content += cell.text.strip() + "\n"
        
        return text_content.strip()
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù DOCX: {e}")
        return ""

def slice_jobs_from_source(source_text: str) -> list:
    """Extract job blocks from source text using flexible patterns."""
    if not source_text:
        return []
    
    # Split by lines and look for job patterns
    lines = source_text.split('\n')
    jobs = []
    current_job = []
    current_job_title = ""
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # Look for job title patterns (more flexible)
        if any(keyword in line for keyword in ['Ù…Ø¯ÙŠØ±', 'Ù…Ø´Ø±Ù', 'Ù…ÙˆØ¸Ù', 'Ù…Ù‡Ù†Ø¯Ø³', 'Ù…Ø­Ù„Ù„', 'Ù…Ø·ÙˆØ±', 'Ù…ØµÙ…Ù…', 'Ù…Ø­Ø§Ø³Ø¨', 'Ù…Ø­Ø§Ù…ÙŠ', 'Ø·Ø¨ÙŠØ¨', 'Ù…Ø¹Ù„Ù…', 'Ù…Ø¯Ø±Ø³']):
            # Save previous job if exists
            if current_job and current_job_title:
                jobs.append({
                    'title': current_job_title,
                    'content': '\n'.join(current_job)
                })
            
            # Start new job
            current_job_title = line
            current_job = [line]
        else:
            # Add line to current job
            current_job.append(line)
    
    # Add the last job
    if current_job and current_job_title:
        jobs.append({
            'title': current_job_title,
            'content': '\n'.join(current_job)
        })
    
    # If no jobs found with strict patterns, try relaxed approach
    if not jobs:
        # Split by double newlines or major separators
        sections = re.split(r'\n\s*\n+', source_text)
        for i, section in enumerate(sections):
            if section.strip():
                lines = section.strip().split('\n')
                if lines:
                    title = lines[0].strip()
                    content = '\n'.join(lines[1:]) if len(lines) > 1 else ""
                    jobs.append({
                        'title': title,
                        'content': content
                    })
    
    return jobs

# Streamlit UI
st.set_page_config(
    page_title="Ù†Ø¸Ø§Ù… Ù…Ù„Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù‡Ù†ÙŠØ©",
    page_icon="ğŸ“‹",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Custom CSS for professional styling
st.markdown("""
<style>
    .main-header {
        text-align: center;
        color: #1f4e79;
        font-size: 2.5rem;
        font-weight: 700;
        margin-bottom: 2rem;
        padding: 1rem;
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        border-radius: 15px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    
    .upload-section {
        background: white;
        padding: 2rem;
        border-radius: 15px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        margin: 1rem 0;
        border-left: 5px solid #007bff;
    }
    
    .mode-selector {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        text-align: center;
    }
    
    .success-box {
        background: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #c3e6cb;
        margin: 1rem 0;
    }
    
    .info-box {
        background: #d1ecf1;
        color: #0c5460;
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #bee5eb;
        margin: 1rem 0;
    }
    
    .download-section {
        background: #e8f5e8;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        text-align: center;
    }
    
    .stButton > button {
        background: linear-gradient(135deg, #007bff 0%, #0056b3 100%);
        color: white;
        border: none;
        padding: 0.75rem 2rem;
        border-radius: 25px;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(0,0,0,0.15);
    }
    
    .stDownloadButton > button {
        background: linear-gradient(135deg, #28a745 0%, #1e7e34 100%);
        color: white;
        border: none;
        padding: 0.75rem 2rem;
        border-radius: 25px;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    
    .stDownloadButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(0,0,0,0.15);
    }
</style>
""", unsafe_allow_html=True)

# Main header
st.markdown('<div class="main-header">Ù†Ø¸Ø§Ù… Ù…Ù„Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù‡Ù†ÙŠØ©</div>', unsafe_allow_html=True)

# Mode selector
with st.container():
    st.markdown('<div class="mode-selector">', unsafe_allow_html=True)
    mode = st.radio(
        "Ø§Ø®ØªØ± ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:",
        ["Multi-Job", "Single Job"],
        horizontal=True,
        label_visibility="collapsed"
    )
    st.markdown('</div>', unsafe_allow_html=True)

# Template upload section
with st.container():
    st.markdown('<div class="upload-section">', unsafe_allow_html=True)
    st.markdown("### ğŸ“„ Ø±ÙØ¹ Ø§Ù„Ù‚Ø§Ù„Ø¨")
    
    template_file = st.file_uploader(
        "Ø§Ø±ÙØ¹ Ù‚Ø§Ù„Ø¨ DOCX",
        type=['docx'],
        help="Ø§Ø±ÙØ¹ Ù‚Ø§Ù„Ø¨ DOCX ÙØ§Ø±Øº Ø£Ùˆ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø£Ø³Ø§Ø³ÙŠ"
    )
    
    if template_file:
        st.markdown('<div class="success-box">âœ… ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø¨Ù†Ø¬Ø§Ø­</div>', unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)

# Data source upload section
with st.container():
    st.markdown('<div class="upload-section">', unsafe_allow_html=True)
    st.markdown("### ğŸ“Š Ø±ÙØ¹ Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    
    src_file = st.file_uploader(
        "Ø§Ø±ÙØ¹ Ù…Ù„Ù Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
        type=['docx', 'json', 'csv'],
        help="Ø§Ø±ÙØ¹ Ù…Ù„Ù DOCX ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ Ø£Ùˆ Ù…Ù„Ù JSON/CSV"
    )
    
    if src_file:
        st.markdown('<div class="success-box">âœ… ØªÙ… Ø±ÙØ¹ Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­</div>', unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)

# Processing and download section
if template_file and src_file:
    with st.container():
        st.markdown('<div class="download-section">', unsafe_allow_html=True)
        st.markdown("### ğŸš€ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        if st.button("Ø§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©", type="primary"):
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
                try:
                    # Load data based on file type
                    if src_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                        # DOCX source file
                        source_text = read_docx_paragraphs(src_file.read())
                        jobs = slice_jobs_from_source(source_text)
                        
                        if not jobs:
                            st.error("Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£ÙŠ ÙˆØ¸Ø§Ø¦Ù ÙÙŠ Ù…Ù„Ù Ø§Ù„Ù…ØµØ¯Ø±")
                            st.stop()
                        
                        # Limit jobs based on mode
                        if mode == "Single Job":
                            jobs = jobs[:1]
                        
                        st.success(f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(jobs)} ÙˆØ¸ÙŠÙØ©")
                        
                        # Process each job
                        filled_docs = {}
                        for i, job in enumerate(jobs):
                            job_title = job['title']
                            job_content = job['content']
                            
                            # Extract data from job content
                            data = {
                                "ref": job_content,
                                "summary": job_content,
                                "channels": job_content,
                                "levels": job_content,
                                "competencies": job_content,
                                "kpis": job_content,
                                "tasks": job_content
                            }
                            
                            # Generate filled document
                            filled_doc = build_filled_docx_bytes(
                                template_file.read(),
                                job_title,
                                data
                            )
                            
                            # Create filename
                            filename = f"Ù†Ù…ÙˆØ°Ø¬_Ù…Ù…Ù„ÙˆØ¡_{i+1}_{job_title[:30]}.docx"
                            filled_docs[filename] = filled_doc
                        
                        # Download options
                        if len(filled_docs) == 1:
                            # Single file download
                            filename = list(filled_docs.keys())[0]
                            st.download_button(
                                label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù…Ù„ÙˆØ¡",
                                data=filled_docs[filename],
                                file_name=filename,
                                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                            )
                        else:
                            # Multiple files - ZIP download
                            zip_data = zip_many(filled_docs)
                            st.download_button(
                                label="ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (ZIP)",
                                data=zip_data,
                                file_name="Ù†Ù…Ø§Ø°Ø¬_Ù…Ù…Ù„ÙˆØ¡Ø©.zip",
                                mime="application/zip"
                            )
                            
                            # Individual file downloads
                            st.markdown("**Ø£Ùˆ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ÙƒÙ„ Ù…Ù„Ù Ø¹Ù„Ù‰ Ø­Ø¯Ø©:**")
                            for filename, doc_data in filled_docs.items():
                                st.download_button(
                                    label=f"ğŸ“¥ {filename}",
                                    data=doc_data,
                                    file_name=filename,
                                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                                )
                    
                    else:
                        # JSON/CSV source files
                        st.info("Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª JSON/CSV Ù‚ÙŠØ¯ Ø§Ù„ØªØ·ÙˆÙŠØ±")
                
                except Exception as e:
                    st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {e}")
        
        st.markdown('</div>', unsafe_allow_html=True)

# Footer
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: #6c757d; padding: 1rem;'>"
    "Ù†Ø¸Ø§Ù… Ù…Ù„Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù‡Ù†ÙŠØ© - Ø¥ØµØ¯Ø§Ø± 2.0 | ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Streamlit"
    "</div>",
    unsafe_allow_html=True
)