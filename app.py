import streamlit as st
import io
import zipfile
from docx import Document
import re

# Utility functions for parsing and cleaning
def normalize_digits(s):
    """Convert Arabic-Indic digits to ASCII digits."""
    if not s:
        return s
    
    # Arabic-Indic to ASCII mapping
    arabic_digits = {
        'Ù ': '0', 'Ù¡': '1', 'Ù¢': '2', 'Ù£': '3', 'Ù¤': '4',
        'Ù¥': '5', 'Ù¦': '6', 'Ù§': '7', 'Ù¨': '8', 'Ù©': '9'
    }
    
    for arabic, ascii_digit in arabic_digits.items():
        s = s.replace(arabic, ascii_digit)
    
    return s

def clean_value(s):
    """Clean and normalize values by removing duplicates, prefixes, and normalizing formatting."""
    if not s:
        return ""
    
    # Normalize digits
    s = normalize_digits(s)
    
    # Split by lines and clean each line
    lines = s.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Remove repeated prefixes like "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:" etc.
        prefixes_to_remove = [
            r'^(\s*Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©\s*[:ï¼š]\s*)+',
            r'^(\s*Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙˆØ­Ø¯Ø§Øª\s*[:ï¼š]\s*)+',
            r'^(\s*Ø±Ù…Ø² Ø§Ù„ÙˆØ­Ø¯Ø§Øª\s*[:ï¼š]\s*)+',
            r'^(\s*Ø§Ù„Ù…Ù‡Ù†Ø©\s*[:ï¼š]\s*)+',
            r'^(\s*Ø±Ù…Ø² Ø§Ù„Ù…Ù‡Ù†Ø©\s*[:ï¼š]\s*)+',
            r'^(\s*Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ù…Ù„\s*[:ï¼š]\s*)+',
            r'^(\s*Ø§Ù„Ù…Ø±ØªØ¨Ø©\s*[:ï¼š]\s*)+'
        ]
        
        for prefix_pattern in prefixes_to_remove:
            line = re.sub(prefix_pattern, '', line)
        
        # Normalize separators (Arabic semicolons/commas)
        line = line.replace('Ø›', ';').replace('ØŒ', ',')
        
        # Collapse multiple spaces
        line = re.sub(r'\s+', ' ', line).strip()
        
        if line and line not in cleaned_lines:
            cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

def extract_reference_data(text):
    """Extract reference data for 'Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ù„Ù„Ù…Ù‡Ù†Ø©' section."""
    expected_labels = [
        "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©", "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©",
        "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©", "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©", "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙˆØ­Ø¯Ø§Øª", "Ø±Ù…Ø² Ø§Ù„ÙˆØ­Ø¯Ø§Øª",
        "Ø§Ù„Ù…Ù‡Ù†Ø©", "Ø±Ù…Ø² Ø§Ù„Ù…Ù‡Ù†Ø©", "Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ù…Ù„", "Ø§Ù„Ù…Ø±ØªØ¨Ø©"
    ]
    
    # Build regex pattern for all labels
    label_pattern = '|'.join(map(re.escape, expected_labels))
    pattern = rf'^\s*({label_pattern})\s*[:ï¼š]\s*(.+)$'
    
    extracted_data = {}
    lines = text.split('\n')
    
    for line in lines:
        match = re.match(pattern, line)
        if match:
            label = match.group(1)
            value = clean_value(match.group(2))
            if label not in extracted_data and value:
                extracted_data[label] = value
    
    # Ensure all expected labels exist (fill with empty if missing)
    result = {}
    for label in expected_labels:
        result[label] = extracted_data.get(label, "")
    
    return result

def extract_communication_channels(text):
    """Extract internal and external communication channels."""
    internal_channels = []
    external_channels = []
    
    lines = text.split('\n')
    current_section = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        if 'Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©' in line or 'Ø¯Ø§Ø®Ù„ÙŠ' in line:
            current_section = 'internal'
            # Extract channels from this line
            channels_text = re.sub(r'^.*?[:ï¼š]\s*', '', line)
            if channels_text:
                channels = [c.strip() for c in re.split(r'[,ØŒ;Ø›]', channels_text) if c.strip()]
                internal_channels.extend(channels)
        elif 'Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©' in line or 'Ø®Ø§Ø±Ø¬ÙŠ' in line:
            current_section = 'external'
            # Extract channels from this line
            channels_text = re.sub(r'^.*?[:ï¼š]\s*', '', line)
            if channels_text:
                channels = [c.strip() for c in re.split(r'[,ØŒ;Ø›]', channels_text) if c.strip()]
                external_channels.extend(channels)
        elif 'Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„ØªÙˆØ§ØµÙ„' in line and current_section:
            purpose_text = re.sub(r'^.*?[:ï¼š]\s*', '', line)
            if purpose_text:
                purpose = clean_value(purpose_text)
                if current_section == 'internal' and internal_channels:
                    # Associate purpose with internal channels
                    pass  # Will be handled in the table creation
                elif current_section == 'external' and external_channels:
                    # Associate purpose with external channels
                    pass  # Will be handled in the table creation
    
    return {
        'internal': internal_channels,
        'external': external_channels
    }

def extract_profession_levels(text):
    """Extract profession levels information."""
    levels_data = {}
    
    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        if 'Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†Ø©' in line:
            levels_data['Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ'] = clean_value(re.sub(r'^.*?[:ï¼š]\s*', '', line))
        elif 'Ø±Ù…Ø² Ø§Ù„Ù…Ø³ØªÙˆÙ‰' in line:
            levels_data['Ø±Ù…Ø² Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†ÙŠ'] = clean_value(re.sub(r'^.*?[:ï¼š]\s*', '', line))
        elif 'Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ' in line:
            levels_data['Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ'] = clean_value(re.sub(r'^.*?[:ï¼š]\s*', '', line))
        elif 'Ø§Ù„ØªØ±ØªÙŠØ¨' in line or 'Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„Ù…Ù‡Ù†ÙŠ' in line:
            levels_data['Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„Ù…Ù‡Ù†ÙŠ (Ø§Ù„Ù…Ø±ØªØ¨Ø©)'] = clean_value(re.sub(r'^.*?[:ï¼š]\s*', '', line))
    
    # Ensure all expected fields exist
    expected_fields = ['Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ', 'Ø±Ù…Ø² Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†ÙŠ', 'Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ', 'Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„Ù…Ù‡Ù†ÙŠ (Ø§Ù„Ù…Ø±ØªØ¨Ø©)']
    for field in expected_fields:
        if field not in levels_data:
            levels_data[field] = ""
    
    return levels_data

def extract_competencies(text):
    """Extract competencies split by type."""
    competencies = {
        'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©': [],
        'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©': [],
        'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©': [],
        'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©': []
    }
    
    lines = text.split('\n')
    current_type = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Determine competency type
        if 'Ø³Ù„ÙˆÙƒÙŠØ©' in line:
            current_type = 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©'
        elif 'Ø£Ø³Ø§Ø³ÙŠØ©' in line:
            current_type = 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©'
        elif 'Ù‚ÙŠØ§Ø¯ÙŠØ©' in line:
            current_type = 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©'
        elif 'ÙÙ†ÙŠØ©' in line:
            current_type = 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©'
        elif current_type and ':' in line:
            # Extract competencies from this line
            comp_text = re.sub(r'^.*?[:ï¼š]\s*', '', line)
            if comp_text:
                comps = [c.strip() for c in re.split(r'[,ØŒ;Ø›]', comp_text) if c.strip()]
                competencies[current_type].extend(comps)
    
    return competencies

def extract_tasks(text):
    """Extract tasks split by category."""
    tasks = {
        'Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©/Ø§Ù„Ø¥Ø´Ø±Ø§ÙÙŠØ©': [],
        'Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ®ØµØµÙŠØ©': [],
        'Ù…Ù‡Ø§Ù… Ø£Ø®Ø±Ù‰ Ø¥Ø¶Ø§ÙÙŠØ©': []
    }
    
    lines = text.split('\n')
    current_category = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Determine task category
        if 'Ù‚ÙŠØ§Ø¯ÙŠØ©' in line or 'Ø¥Ø´Ø±Ø§ÙÙŠØ©' in line:
            current_category = 'Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©/Ø§Ù„Ø¥Ø´Ø±Ø§ÙÙŠØ©'
        elif 'ØªØ®ØµØµÙŠØ©' in line:
            current_category = 'Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ®ØµØµÙŠØ©'
        elif 'Ù…Ù‡Ø§Ù… Ø£Ø®Ø±Ù‰' in line or 'Ø¥Ø¶Ø§ÙÙŠØ©' in line:
            current_category = 'Ù…Ù‡Ø§Ù… Ø£Ø®Ø±Ù‰ Ø¥Ø¶Ø§ÙÙŠØ©'
        elif current_category and ':' in line:
            # Extract tasks from this line
            task_text = re.sub(r'^.*?[:ï¼š]\s*', '', line)
            if task_text:
                task_list = [t.strip() for t in re.split(r'[,ØŒ;Ø›]', task_text) if t.strip()]
                tasks[current_category].extend(task_list)
    
    return tasks

def extract_kpis(text):
    """Extract KPIs with measurement methods."""
    kpis = []
    
    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Look for KPI patterns like "1- {kpi} - Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³: {method}"
        kpi_match = re.match(r'^(\d+)[-Ù€]\s*(.+?)\s*[-Ù€]\s*Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³\s*[:ï¼š]\s*(.+)$', line)
        if kpi_match:
            kpis.append({
                'Ø§Ù„Ø±Ù‚Ù…': kpi_match.group(1),
                'Ù…Ø¤Ø´Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡': clean_value(kpi_match.group(2)),
                'Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³': clean_value(kpi_match.group(3))
            })
    
    # If no structured KPIs found, try to extract from general text
    if not kpis:
        kpi_text = clean_value(text)
        if kpi_text:
            kpis = [{'Ø§Ù„Ø±Ù‚Ù…': '1', 'Ù…Ø¤Ø´Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡': kpi_text, 'Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³': 'Ù‚ÙŠØ§Ø³ Ù…Ø¨Ø§Ø´Ø±'}]
    
    return kpis

# DOCX building helper functions
def add_title(doc, text):
    """Add a centered, bold title."""
    title = doc.add_heading(text, 0)
    title.alignment = 1  # Center alignment
    return title

def add_keyval_table(doc, title, rows):
    """Add a 2-column key-value table."""
    if not rows:
        return
    
    # Create table
    table = doc.add_table(rows=len(rows) + 1, cols=2)
    table.style = 'Table Grid'
    
    # Header row
    header_cells = table.rows[0].cells
    header_cells[0].text = title
    header_cells[1].text = "Ø§Ù„Ù‚ÙŠÙ…Ø©"
    
    # Make header bold and center
    for cell in header_cells:
        for paragraph in cell.paragraphs:
            paragraph.alignment = 1  # Center
            for run in paragraph.runs:
                run.bold = True
    
    # Data rows
    for i, (key, value) in enumerate(rows.items() if isinstance(rows, dict) else enumerate(rows)):
        row_cells = table.rows[i + 1].cells
        if isinstance(rows, dict):
            row_cells[0].text = str(key)
            row_cells[1].text = str(value) if value else ""
        else:
            row_cells[0].text = str(key)
            row_cells[1].text = str(value) if value else ""
        
        # Make label column bold
        for paragraph in row_cells[0].paragraphs:
            for run in paragraph.runs:
                run.bold = True
    
    # Set RTL alignment for all cells
    for row in table.rows:
        for cell in row.cells:
            for paragraph in cell.paragraphs:
                paragraph.alignment = 2  # RTL alignment
    
    # Add spacing after table
    doc.add_paragraph("")

def add_list_table(doc, title, items):
    """Add a single-column list table."""
    if not items:
        return
    
    # Create table
    table = doc.add_table(rows=len(items) + 1, cols=1)
    table.style = 'Table Grid'
    
    # Header row
    header_cell = table.rows[0].cells[0]
    header_cell.text = title
    
    # Make header bold and center
    for paragraph in header_cell.paragraphs:
        paragraph.alignment = 1  # Center
        for run in paragraph.runs:
            run.bold = True
    
    # Data rows
    for i, item in enumerate(items):
        row_cell = table.rows[i + 1].cells[0]
        row_cell.text = str(item)
    
    # Set RTL alignment for all cells
    for row in table.rows:
        for cell in row.cells:
            for paragraph in cell.paragraphs:
                paragraph.alignment = 2  # RTL alignment
    
    # Add spacing after table
    doc.add_paragraph("")

def add_two_col_table(doc, title, rows):
    """Add a two-column table for communication channels."""
    if not rows:
        return
    
    # Create table
    table = doc.add_table(rows=len(rows) + 1, cols=2)
    table.style = 'Table Grid'
    
    # Header row
    header_cells = table.rows[0].cells
    header_cells[0].text = title
    header_cells[1].text = "Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„ØªÙˆØ§ØµÙ„"
    
    # Make header bold and center
    for cell in header_cells:
        for paragraph in cell.paragraphs:
            paragraph.alignment = 1  # Center
            for run in paragraph.runs:
                run.bold = True
    
    # Data rows
    for i, row_data in enumerate(rows):
        row_cells = table.rows[i + 1].cells
        if isinstance(row_data, dict):
            row_cells[0].text = str(row_data.get('Ø¬Ù‡Ø©', ''))
            row_cells[1].text = str(row_data.get('ØºØ±Ø¶', ''))
        else:
            row_cells[0].text = str(row_data[0]) if len(row_data) > 0 else ""
            row_cells[1].text = str(row_data[1]) if len(row_data) > 1 else ""
    
    # Set RTL alignment for all cells
    for row in table.rows:
        for cell in row.cells:
            for paragraph in cell.paragraphs:
                paragraph.alignment = 2  # RTL alignment
    
    # Add spacing after table
    doc.add_paragraph("")

def add_kpi_table(doc, title, rows):
    """Add a 3-column KPI table."""
    if not rows:
        return
    
    # Create table
    table = doc.add_table(rows=len(rows) + 1, cols=3)
    table.style = 'Table Grid'
    
    # Header row
    header_cells = table.rows[0].cells
    header_cells[0].text = "Ø§Ù„Ø±Ù‚Ù…"
    header_cells[1].text = "Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"
    header_cells[2].text = "Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³"
    
    # Make header bold and center
    for cell in header_cells:
        for paragraph in cell.paragraphs:
            paragraph.alignment = 1  # Center
            for run in paragraph.runs:
                run.bold = True
    
    # Data rows
    for i, row_data in enumerate(rows):
        row_cells = table.rows[i + 1].cells
        if isinstance(row_data, dict):
            row_cells[0].text = str(row_data.get('Ø§Ù„Ø±Ù‚Ù…', i + 1))
            row_cells[1].text = str(row_data.get('Ù…Ø¤Ø´Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡', ''))
            row_cells[2].text = str(row_data.get('Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³', ''))
        else:
            row_cells[0].text = str(i + 1)
            row_cells[1].text = str(row_data[0]) if len(row_data) > 0 else ""
            row_cells[2].text = str(row_data[1]) if len(row_data) > 1 else ""
    
    # Set RTL alignment for all cells
    for row in table.rows:
        for cell in row.cells:
            for paragraph in cell.paragraphs:
                paragraph.alignment = 2  # RTL alignment
    
    # Add spacing after table
    doc.add_paragraph("")

def build_filled_docx_bytes(template_bytes: bytes, job_title: str, data: dict) -> bytes:
    """
    Build a filled DOCX using structured tables following the exact template design.
    This creates clean, professional documents with proper table structures.
    """
    try:
        # Create a new document
        doc = Document()
        
        # Add title - centered and bold
        title = add_title(doc, f"Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ÙˆØµÙ Ø§Ù„Ù…Ù‡Ù†ÙŠ â€” {job_title}")
        
        # Add spacing after title
        doc.add_paragraph("")
        
        # Section 1: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ù„Ù„Ù…Ù‡Ù†Ø© (Job Reference Data)
        ref_data = extract_reference_data(data.get("ref", ""))
        # Override with job title
        ref_data["Ø§Ù„Ù…Ù‡Ù†Ø©"] = job_title
        add_keyval_table(doc, "1- Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ù„Ù„Ù…Ù‡Ù†Ø©", ref_data)
        
        # Section 2: Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù…Ù‡Ù†Ø© (General Summary)
        summary_data = clean_value(data.get("summary", "")) or "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ø®Øµ"
        add_keyval_table(doc, "2- Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù…Ù‡Ù†Ø©", {"Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù…": summary_data})
        
        # Section 3: Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ (Communication Channels)
        channels = extract_communication_channels(data.get("channels", ""))
        
        # Internal channels table
        if channels['internal']:
            internal_rows = [{'Ø¬Ù‡Ø©': channel, 'ØºØ±Ø¶': 'ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„'} for channel in channels['internal']]
            add_two_col_table(doc, "3- Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©", internal_rows)
        
        # External channels table
        if channels['external']:
            external_rows = [{'Ø¬Ù‡Ø©': channel, 'ØºØ±Ø¶': 'Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡'} for channel in channels['external']]
            add_two_col_table(doc, "3- Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©", external_rows)
        
        # Section 4: Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ© (Standard Profession Levels)
        levels_data = extract_profession_levels(data.get("levels", ""))
        add_keyval_table(doc, "4- Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ©", levels_data)
        
        # Section 5: Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª (Competencies)
        competencies = extract_competencies(data.get("competencies", ""))
        for comp_type, comp_list in competencies.items():
            if comp_list:
                add_list_table(doc, f"5- {comp_type}", comp_list)
        
        # Section 6: Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù†ÙŠ (Performance Management)
        kpis = extract_kpis(data.get("kpis", ""))
        if kpis:
            add_kpi_table(doc, "6- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù†ÙŠ", kpis)
        
        # Section 7: Ø§Ù„Ù…Ù‡Ø§Ù… (Tasks)
        tasks = extract_tasks(data.get("tasks", ""))
        for task_type, task_list in tasks.items():
            if task_list:
                add_list_table(doc, f"7- {task_type}", task_list)
        
        # Add Form B: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙˆØµÙ Ø§Ù„ÙØ¹Ù„ÙŠ (Actual Description Form)
        doc.add_heading("Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙˆØµÙ Ø§Ù„ÙØ¹Ù„ÙŠ", level=1)
        doc.add_paragraph("")
        
        # Form B Section 1: Ø§Ù„Ù…Ù‡Ø§Ù… (Tasks)
        if tasks:
            for task_type, task_list in tasks.items():
                if task_list:
                    add_list_table(doc, f"1- {task_type}", task_list)
        
        # Form B Section 2: Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ© ÙˆØ§Ù„ÙÙ†ÙŠØ© (Behavioral and Technical Competencies)
        behavioral_comps = competencies.get('Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©', [])
        technical_comps = competencies.get('Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©', [])
        
        if behavioral_comps:
            behavioral_rows = [{'Ø§Ù„Ø±Ù‚Ù…': i+1, 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©': comp, 'Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†': 'Ù…ØªÙ‚Ø¯Ù…'} 
                             for i, comp in enumerate(behavioral_comps[:5])]
            add_kpi_table(doc, "2- Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ© ÙˆØ§Ù„ÙÙ†ÙŠØ©", behavioral_rows)
        
        if technical_comps:
            technical_rows = [{'Ø§Ù„Ø±Ù‚Ù…': i+1, 'Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©': comp, 'Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†': 'Ù…ØªÙ‚Ø¯Ù…'} 
                            for i, comp in enumerate(technical_comps[:5])]
            add_kpi_table(doc, "2- Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©", technical_rows)
        
        # Form B Section 3: Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù†ÙŠ (Performance Management)
        if kpis:
            add_kpi_table(doc, "3- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù†ÙŠ", kpis)
        
        # Save the rendered document to bytes
        out = io.BytesIO()
        doc.save(out)
        out.seek(0)
        return out.read()
        
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ù„Ø¡ Ø§Ù„Ù‚Ø§Ù„Ø¨: {e}")
        return template_bytes

st.set_page_config(page_title="Ù…Ù„Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Multi-Job)", layout="centered")
st.title("Ù…Ù„Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ â€” Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù (DOCX â†’ DOCX)")
st.caption("Ù‚Ù… Ø¨Ø±ÙØ¹ Ù‚Ø§Ù„Ø¨ DOCX ÙˆÙ…ØµØ¯Ø± Ø¨ÙŠØ§Ù†Ø§Øª ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø¯Ø© ÙˆØ¸Ø§Ø¦Ù. Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ù…Ù„Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹.")

# Add processing mode selection
processing_mode = st.radio(
    "ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© / Processing Mode:",
    ["Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù / Multi-Job", "ÙˆØ¸ÙŠÙØ© ÙˆØ§Ø­Ø¯Ø© / Single Job"],
    horizontal=True
)

tmpl_file = st.file_uploader("Ø±ÙØ¹ Ø§Ù„Ù‚Ø§Ù„Ø¨ (DOCX) / Upload Template (DOCX)", type=["docx"])
st.info("ğŸ’¡ **Ù‡Ø§Ù…**: Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø³ÙŠÙ‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†Ø§Ø¦Ø¨Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙˆÙ…Ù„Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„")

# Show template upload status
if tmpl_file:
    st.success(f"âœ… ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù‚Ø§Ù„Ø¨: {tmpl_file.name}")

# Define src_file outside the conditional blocks
if processing_mode == "Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù / Multi-Job":
    src_file = st.file_uploader("Ø±ÙØ¹ Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (DOCX) / Upload Data Source (DOCX)", type=["docx"])
    st.info("ğŸ“‹ ÙˆØ¶Ø¹ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù: Ø³ÙŠÙ‚ÙˆÙ… Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ÙƒÙ„ ÙˆØ¸ÙŠÙØ©")
else:
    src_file = st.file_uploader("Ø±ÙØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØ¸ÙŠÙØ© (DOCX) / Upload Job Data (DOCX)", type=["docx"])
    st.info("ğŸ“„ ÙˆØ¶Ø¹ ÙˆØ¸ÙŠÙØ© ÙˆØ§Ø­Ø¯Ø©: Ø³ÙŠÙ‚ÙˆÙ… Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")

# Show source file upload status
if src_file:
    st.success(f"âœ… ØªÙ… Ø±ÙØ¹ Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {src_file.name}")

# ---------- helpers ----------
def create_template_structure():
    """
    Create the standard template structure with placeholders
    """
    template_structure = {
        "Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ÙˆØµÙ Ø§Ù„Ù…Ù‡Ù†ÙŠ": {
            "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ù„Ù„Ù…Ù‡Ù†Ø©": {
                "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©": "{{main_group}}",
                "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©": "{{main_group_code}}",
                "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©": "{{sub_group}}",
                "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©": "{{sub_group_code}}",
                "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©": "{{secondary_group}}",
                "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©": "{{secondary_group_code}}",
                "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙˆØ­Ø¯Ø§Øª": "{{units_group}}",
                "Ø±Ù…Ø² Ø§Ù„ÙˆØ­Ø¯Ø§Øª": "{{units_code}}",
                "Ø§Ù„Ù…Ù‡Ù†Ø©": "{{profession}}",
                "Ø±Ù…Ø² Ø§Ù„Ù…Ù‡Ù†Ø©": "{{profession_code}}",
                "Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ù…Ù„": "{{work_location}}",
                "Ø§Ù„Ù…Ø±ØªØ¨Ø©": "{{rank}}"
            },
            "Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù…Ù‡Ù†Ø©": "{{summary}}",
            "Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„": {
                "Ø¬Ù‡Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©": [
                    {
                        "Ø§Ù„Ø¬Ù‡Ø©": "{{internal_party_1}}",
                        "Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„ØªÙˆØ§ØµÙ„": "{{internal_purpose_1}}"
                    }
                ],
                "Ø¬Ù‡Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©": [
                    {
                        "Ø§Ù„Ø¬Ù‡Ø©": "{{external_party_1}}",
                        "Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„ØªÙˆØ§ØµÙ„": "{{external_purpose_1}}"
                    }
                ]
            },
            "Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ©": [
                {
                    "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ": "{{level_1}}",
                    "Ø±Ù…Ø² Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‡Ù†ÙŠ": "{{level_code_1}}",
                    "Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ": "{{role_1}}",
                    "Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„Ù…Ù‡Ù†ÙŠ (Ø§Ù„Ù…Ø±ØªØ¨Ø©)": "{{progression_1}}"
                }
            ],
            "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª": {
                "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©": ["{{behavioral_comp_1}}", "{{behavioral_comp_2}}", "{{behavioral_comp_3}}"],
                "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©": ["{{core_comp_1}}", "{{core_comp_2}}", "{{core_comp_3}}"],
                "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©": ["{{leadership_comp_1}}", "{{leadership_comp_2}}", "{{leadership_comp_3}}"],
                "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©": ["{{technical_comp_1}}", "{{technical_comp_2}}", "{{technical_comp_3}}"]
            }
        },
        "Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙˆØµÙ Ø§Ù„ÙØ¹Ù„ÙŠ": {
            "Ø§Ù„Ù…Ù‡Ø§Ù…": {
                "Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©/Ø§Ù„Ø¥Ø´Ø±Ø§ÙÙŠØ©": ["{{leadership_task_1}}", "{{leadership_task_2}}", "{{leadership_task_3}}"],
                "Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ®ØµØµÙŠØ©": ["{{specialized_task_1}}", "{{specialized_task_2}}", "{{specialized_task_3}}"],
                "Ù…Ù‡Ø§Ù… Ø£Ø®Ø±Ù‰ Ø¥Ø¶Ø§ÙÙŠØ©": ["{{additional_task_1}}", "{{additional_task_2}}", "{{additional_task_3}}"]
            },
            "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ© ÙˆØ§Ù„ÙÙ†ÙŠØ©": {
                "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©": [
                    {
                        "Ø§Ù„Ø±Ù‚Ù…": "1",
                        "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©": "{{behavioral_comp_1}}",
                        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†": "{{proficiency_1}}"
                    },
                    {
                        "Ø§Ù„Ø±Ù‚Ù…": "2",
                        "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©": "{{behavioral_comp_2}}",
                        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†": "{{proficiency_2}}"
                    },
                    {
                        "Ø§Ù„Ø±Ù‚Ù…": "3",
                        "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©": "{{behavioral_comp_3}}",
                        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†": "{{proficiency_3}}"
                    },
                    {
                        "Ø§Ù„Ø±Ù‚Ù…": "4",
                        "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©": "{{behavioral_comp_4}}",
                        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†": "{{proficiency_4}}"
                    },
                    {
                        "Ø§Ù„Ø±Ù‚Ù…": "5",
                        "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©": "{{behavioral_comp_5}}",
                        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†": "{{proficiency_5}}"
                    }
                ],
                "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©": [
                    {
                        "Ø§Ù„Ø±Ù‚Ù…": "1",
                        "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©": "{{technical_comp_1}}",
                        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†": "{{proficiency_1}}"
                    },
                    {
                        "Ø§Ù„Ø±Ù‚Ù…": "2",
                        "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©": "{{technical_comp_2}}",
                        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†": "{{proficiency_2}}"
                    },
                    {
                        "Ø§Ù„Ø±Ù‚Ù…": "3",
                        "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©": "{{technical_comp_3}}",
                        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†": "{{proficiency_3}}"
                    },
                    {
                        "Ø§Ù„Ø±Ù‚Ù…": "4",
                        "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©": "{{technical_comp_4}}",
                        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†": "{{proficiency_4}}"
                    },
                    {
                        "Ø§Ù„Ø±Ù‚Ù…": "5",
                        "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø©": "{{technical_comp_5}}",
                        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†": "{{proficiency_5}}"
                    }
                ]
            },
            "Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù†ÙŠ": [
                {
                    "Ø§Ù„Ø±Ù‚Ù…": "1",
                    "Ù…Ø¤Ø´Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ": "{{kpi_1}}",
                    "Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³": "{{measurement_1}}"
                },
                {
                    "Ø§Ù„Ø±Ù‚Ù…": "2",
                    "Ù…Ø¤Ø´Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ": "{{kpi_2}}",
                    "Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³": "{{measurement_2}}"
                },
                {
                    "Ø§Ù„Ø±Ù‚Ù…": "3",
                    "Ù…Ø¤Ø´Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ": "{{kpi_3}}",
                    "Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³": "{{measurement_3}}"
                },
                {
                    "Ø§Ù„Ø±Ù‚Ù…": "4",
                    "Ù…Ø¤Ø´Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ": "{{kpi_4}}",
                    "Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³": "{{measurement_4}}"
                }
            ]
        }
    }
    return template_structure

def create_template_with_placeholders(template_bytes: bytes) -> bytes:
    """
    Create a new template with placeholders based on the standard structure
    """
    try:
        # Create a new document with placeholders
        doc = Document()
        
        # Add title
        title = doc.add_heading("Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ÙˆØµÙ Ø§Ù„Ù…Ù‡Ù†ÙŠ", 0)
        
        # Add sections with placeholders
        sections = [
            ("1- Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ù„Ù„Ù…Ù‡Ù†Ø©", [
                "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {{main_group}}",
                "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {{main_group_code}}",
                "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©: {{sub_group}}",
                "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©: {{sub_group_code}}",
                "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©: {{secondary_group}}",
                "Ø±Ù…Ø² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©: {{secondary_group_code}}",
                "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙˆØ­Ø¯Ø§Øª: {{units_group}}",
                "Ø±Ù…Ø² Ø§Ù„ÙˆØ­Ø¯Ø§Øª: {{units_code}}",
                "Ø§Ù„Ù…Ù‡Ù†Ø©: {{profession}}",
                "Ø±Ù…Ø² Ø§Ù„Ù…Ù‡Ù†Ø©: {{profession_code}}",
                "Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ù…Ù„: {{work_location}}",
                "Ø§Ù„Ù…Ø±ØªØ¨Ø©: {{rank}}"
            ]),
            ("2- Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù…Ù‡Ù†Ø©", ["{{summary}}"]),
            ("3- Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„", [
                "Ø¬Ù‡Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©: {{internal_party_1}} - {{internal_purpose_1}}",
                "Ø¬Ù‡Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©: {{external_party_1}} - {{external_purpose_1}}"
            ]),
            ("4- Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ©", [
                "Ø§Ù„Ù…Ø³ØªÙˆÙ‰ 1: {{level_1}} ({{level_code_1}}) - {{role_1}} - {{progression_1}}"
            ]),
            ("5- Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª", [
                "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©: {{behavioral_comp_1}}, {{behavioral_comp_2}}, {{behavioral_comp_3}}",
                "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {{core_comp_1}}, {{core_comp_2}}, {{core_comp_3}}",
                "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©: {{leadership_comp_1}}, {{leadership_comp_2}}, {{leadership_comp_3}}",
                "Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©: {{technical_comp_1}}, {{technical_comp_2}}, {{technical_comp_3}}"
            ])
        ]
        
        for section_title, items in sections:
            doc.add_heading(section_title, level=1)
            for item in items:
                doc.add_paragraph(item)
            doc.add_paragraph("")  # Add space between sections
        
        # Add Form B
        doc.add_heading("Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙˆØµÙ Ø§Ù„ÙØ¹Ù„ÙŠ", level=0)
        
        # Tasks section
        doc.add_heading("1- Ø§Ù„Ù…Ù‡Ø§Ù…", level=1)
        doc.add_paragraph("Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©/Ø§Ù„Ø¥Ø´Ø±Ø§ÙÙŠØ©: {{leadership_task_1}}, {{leadership_task_2}}, {{leadership_task_3}}")
        doc.add_paragraph("Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ®ØµØµÙŠØ©: {{specialized_task_1}}, {{specialized_task_2}}, {{specialized_task_3}}")
        doc.add_paragraph("Ù…Ù‡Ø§Ù… Ø£Ø®Ø±Ù‰ Ø¥Ø¶Ø§ÙÙŠØ©: {{additional_task_1}}, {{additional_task_2}}, {{additional_task_3}}")
        
        # Competencies section
        doc.add_heading("2- Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ© ÙˆØ§Ù„ÙÙ†ÙŠØ©", level=1)
        doc.add_paragraph("Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©:")
        for i in range(1, 6):
            doc.add_paragraph(f"{i}- {{behavioral_comp_{i}}} - Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†: {{proficiency_{i}}}")
        
        doc.add_paragraph("Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©:")
        for i in range(1, 6):
            doc.add_paragraph(f"{i}- {{technical_comp_{i}}} - Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ØªÙ‚Ø§Ù†: {{proficiency_{i}}}")
        
        # Performance section
        doc.add_heading("3- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù†ÙŠ", level=1)
        for i in range(1, 5):
            doc.add_paragraph(f"{i}- {{kpi_{i}}} - Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³: {{measurement_{i}}}")
        
        # Save the new template
        out = io.BytesIO()
        doc.save(out)
        out.seek(0)
        return out.read()
        
    except Exception as e:
        st.error(f"Error creating template: {e}")
        return template_bytes

def read_docx_paragraphs(file_bytes) -> list[str]:
    """
    Read paragraphs from a DOCX file using python-docx Document.
    This function is used to parse the source document for job information.
    """
    try:
        # Use Document for reading source files (more reliable for parsing)
        doc = Document(io.BytesIO(file_bytes))
        
        paras = []
        for p in doc.paragraphs:
            text = (p.text or "").strip()
            if text != "":
                paras.append(text)
        return paras
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù DOCX: {e}")
        return []

def slice_jobs_from_source(paras: list[str], single_job: bool = False) -> dict:
    """
    Heuristic parser for job data:
    - A job block starts at a line that looks like a job title (Arabic words, not starting with digits or bullets),
      and within the next ~6 lines we see a numbered section like '1)'.
    - Sections inside each job: 1) ... 7)
    Returns: { job_title: { 'ref':..., 'summary':..., 'channels':..., 'levels':..., 'competencies':..., 'kpis':..., 'tasks':... } }
    """
    text = "\n".join(paras)

    # Split into candidates by lines that look like headings
    # We'll treat any line without leading digit and with Arabic letters as a potential job start.
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    job_indices = []
    
    # More flexible job detection - look for lines that could be job titles
    for i, line in enumerate(lines):
        # Check if line looks like a job title (has Arabic text, reasonable length, no leading numbers)
        if (len(line) > 3 and 
            re.search(r"[\u0600-\u06FF]", line) and  # Contains Arabic text
            not re.match(r"^\d", line) and            # Doesn't start with number
            not re.match(r"^[â€¢\-\*]", line) and      # Doesn't start with bullet
            not re.match(r"^\s*\d+\)", line)):       # Doesn't start with numbered section
            
            # Look ahead to see if this could be a job section
            # Check if within next 10 lines we have some numbered content
            window_lines = lines[i:i+10]
            window_text = "\n".join(window_lines)
            
            # More flexible pattern matching for numbered sections
            has_numbered_sections = (
                re.search(r"\b1\)", window_text) or           # 1)
                re.search(r"\b2\)", window_text) or           # 2)
                re.search(r"\b3\)", window_text) or           # 3)
                re.search(r"\b4\)", window_text) or           # 4)
                re.search(r"\b5\)", window_text) or           # 5)
                re.search(r"\b6\)", window_text) or           # 6)
                re.search(r"\b7\)", window_text) or           # 7)
                re.search(r"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", window_text) or        # Contains "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"
                re.search(r"Ø§Ù„Ù…Ù„Ø®Øµ", window_text) or          # Contains "Ø§Ù„Ù…Ù„Ø®Øµ"
                re.search(r"Ø§Ù„Ù…Ù‡Ø§Ù…", window_text)             # Contains "Ø§Ù„Ù…Ù‡Ø§Ù…"
            )
            
            if has_numbered_sections:
                job_indices.append(i)

    # If no jobs found with strict criteria, try more relaxed approach
    if not job_indices:
        st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙˆØ¸Ø§Ø¦Ù Ø¨Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØµØ§Ø±Ù…Ø©. Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø£ÙƒØ«Ø± Ù…Ø±ÙˆÙ†Ø©...")
        
        # Look for any line with Arabic text that could be a job title
        for i, line in enumerate(lines):
            if (len(line) > 2 and 
                re.search(r"[\u0600-\u06FF]", line) and  # Contains Arabic text
                not re.match(r"^\d", line) and            # Doesn't start with number
                not re.match(r"^[â€¢\-\*]", line)):         # Doesn't start with bullet
                
                # Check if this line is followed by content (not just empty lines)
                next_lines = lines[i+1:i+5]
                if any(len(l.strip()) > 0 for l in next_lines):
                    job_indices.append(i)

    # Add end sentinel
    job_indices = sorted(set(job_indices))
    blocks = {}
    
    # For single job mode, only process the first job
    if single_job and job_indices:
        job_indices = job_indices[:1]
    
    for idx, start in enumerate(job_indices):
        end = job_indices[idx+1] if idx+1 < len(job_indices) else len(lines)
        chunk = "\n".join(lines[start:end]).strip()
        if not chunk:
            continue
        # Job title = first line
        job_title = lines[start]
        # Extract numbered sections
        def cap(pattern):
            m = re.search(pattern, chunk, re.S)
            return m.group(1).strip() if m else ""

        # More flexible pattern matching for sections
        ref_block      = cap(r"1\)\s*Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.*?\n(.*?)(?=\n\d\)|\Z)") or cap(r"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.*?\n(.*?)(?=\n\d\)|\Z)")
        summary_block  = cap(r"2\)\s*Ø§Ù„Ù…Ù„Ø®Øµ.*?\n(.*?)(?=\n\d\)|\Z)") or cap(r"Ø§Ù„Ù…Ù„Ø®Øµ.*?\n(.*?)(?=\n\d\)|\Z)")
        channels_block = cap(r"3\)\s*Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„.*?\n(.*?)(?=\n\d\)|\Z)") or cap(r"Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„.*?\n(.*?)(?=\n\d\)|\Z)")
        levels_block   = cap(r"4\)\s*Ù…Ø³ØªÙˆÙŠØ§Øª.*?\n(.*?)(?=\n\d\)|\Z)") or cap(r"Ù…Ø³ØªÙˆÙŠØ§Øª.*?\n(.*?)(?=\n\d\)|\Z)")
        comp_block     = cap(r"5\)\s*Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª.*?\n(.*?)(?=\n\d\)|\Z)") or cap(r"Ø§Ù„Ø¬Ø¯Ø§Ø±Ø§Øª.*?\n(.*?)(?=\n\d\)|\Z)")
        kpis_block     = cap(r"6\)\s*Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡.*?\n(.*?)(?=\n\d\)|\Z)") or cap(r"Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡.*?\n(.*?)(?=\n\d\)|\Z)")
        tasks_block    = cap(r"7\)\s*Ø§Ù„Ù…Ù‡Ø§Ù….*?\n(.*?)(?=\n\d\)|\Z)") or cap(r"Ø§Ù„Ù…Ù‡Ø§Ù….*?\n(.*?)(?=\n\d\)|\Z)")

        blocks[job_title] = {
            "ref": ref_block,
            "summary": summary_block,
            "channels": channels_block,
            "levels": levels_block,
            "competencies": comp_block,
            "kpis": kpis_block,
            "tasks": tasks_block
        }

    return blocks

def zip_many(named_bytes: dict[str, bytes]) -> bytes:
    bio = io.BytesIO()
    with zipfile.ZipFile(bio, "w", compression=zipfile.ZIP_DEFLATED) as z:
        for fname, b in named_bytes.items():
            z.writestr(fname, b)
    bio.seek(0)
    return bio.read()

# ---------- main ----------
if st.button("Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù…Ù„ÙˆØ¡Ø© / Generate Filled Forms", type="primary", disabled=(tmpl_file is None or src_file is None)):
    try:
        tmpl_bytes = tmpl_file.read()
        src_bytes  = src_file.read()

        # Show template structure
        st.write("**ğŸ“‹ Template Structure Created:**")
        template_structure = create_template_structure()
        st.json(template_structure)
        
        # Create template with placeholders
        st.write("**ğŸ”§ Creating template with placeholders...**")
        template_with_placeholders = create_template_with_placeholders(tmpl_bytes)
        
        # Show download for template with placeholders
        st.download_button(
            "ğŸ“¥ Download Template with Placeholders",
            data=template_with_placeholders,
            file_name="template_with_placeholders.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )

        # parse jobs
        paras = read_docx_paragraphs(src_bytes)
        single_job_mode = processing_mode == "ÙˆØ¸ÙŠÙØ© ÙˆØ§Ø­Ø¯Ø© / Single Job"
        jobs = slice_jobs_from_source(paras, single_job_mode)

        if not jobs:
            st.error("Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£ÙŠ ÙˆØ¸Ø§Ø¦Ù. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù…Ù„Ù Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª DOCX ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ Ø£Ù‚Ø³Ø§Ù… Ù…Ø±Ù‚Ù…Ø© Ø£Ùˆ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø«Ù„ 'Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª'ØŒ 'Ø§Ù„Ù…Ù„Ø®Øµ'ØŒ 'Ø§Ù„Ù…Ù‡Ø§Ù…'.")
        else:
            if single_job_mode:
                st.success(f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙˆØ¸ÙŠÙØ© ÙˆØ§Ø­Ø¯Ø©. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
            else:
                st.success(f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(jobs)} ÙˆØ¸ÙŠÙØ©(ÙˆØ¸Ø§Ø¦Ù). Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...")
            
            files = {}
            for job_title, data in jobs.items():
                doc_bytes = build_filled_docx_bytes(template_with_placeholders, job_title, data)
                safe_name = re.sub(r'[\\/*?:"<>|]', "-", job_title)
                files[f"{safe_name}.docx"] = doc_bytes
                st.download_button(f"ØªØ­Ù…ÙŠÙ„: {job_title} / Download: {job_title}", data=doc_bytes, file_name=f"{safe_name}.docx", mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")

            # zip all (only for multi-job mode)
            if not single_job_mode and len(files) > 1:
                zip_bytes = zip_many(files)
                st.download_button("ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙ„ (ZIP) / Download ALL (ZIP)", data=zip_bytes, file_name="filled_jobs.zip", mime="application/zip")

    except Exception as e:
        st.error(f"Ø®Ø·Ø£: {e}")

st.markdown("""
**Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø© / Important Notes**

## ğŸ¯ **ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¢Ù† / How the App Works Now:**

1. **ÙŠØ±ÙØ¹ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù‚Ø§Ù„Ø¨** (Ø£ÙŠ Ù‚Ø§Ù„Ø¨ DOCX)
2. **Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠÙ†Ø´Ø¦ Ù‚Ø§Ù„Ø¨ Ø¬Ø¯ÙŠØ¯** Ù…Ø¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†Ø§Ø¦Ø¨Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
3. **ÙŠØ±ÙØ¹ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ù…ØµØ¯Ø±** Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
4. **Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠÙ…Ù„Ø£ Ø§Ù„Ù‚Ø§Ù„Ø¨** Ø¨Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ„ ÙˆØ¸ÙŠÙØ©
5. **ÙŠØ­ØµÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ù…Ù…Ù„ÙˆØ¡Ø©** Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…

## âœ… **Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:**
- **Ø¥Ù†Ø´Ø§Ø¡ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†Ø§Ø¦Ø¨Ø©** - Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„Ø¥Ø¶Ø§ÙØªÙ‡Ø§ ÙŠØ¯ÙˆÙŠØ§Ù‹
- **Ù…Ù„Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„ÙØ§Ø±ØºØ©** ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
- **Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©**
- **Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø¯Ø© ÙˆØ¸Ø§Ø¦Ù** ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª
- **ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù„Ø¨ Ù…Ø¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†Ø§Ø¦Ø¨Ø©** Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ

## ğŸ”§ **Ø§Ù„Ø¹Ù…Ù„ÙŠØ©:**
1. Ø§Ø±ÙØ¹ Ù‚Ø§Ù„Ø¨Ùƒ
2. Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ù„Ø¨ Ù…Ø¹ Ø¹Ù†Ø§ØµØ± Ù†Ø§Ø¦Ø¨Ø©
3. Ø§Ø±ÙØ¹ Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
4. Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬ Ù…Ù…Ù„ÙˆØ¡Ø© Ø¬Ø§Ù‡Ø²Ø©
""")